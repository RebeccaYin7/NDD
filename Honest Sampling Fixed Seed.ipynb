{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siptest\\anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\siptest\\anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\siptest\\anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\siptest\\anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "C:\\Users\\siptest\\anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import njit\n",
    "import pickle\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree._classes import DecisionTreeClassifier\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import entropy, multivariate_normal\n",
    "\n",
    "from hyppo.tools import multimodal_independence, indep_sim\n",
    "from hyppo.ksample._utils import k_sample_transform\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stat_helper(tree, tree_idx, X, y, sampled_indices, unsampled_indices, K, kappa=3, base=2):\n",
    "    # Randomly split the rest into voting and evaluation.\n",
    "\n",
    "    total_unsampled = len(unsampled_indices)\n",
    "    # np.random.shuffle(unsampled_indices)\n",
    "    vote_indices = unsampled_indices[:total_unsampled//2]\n",
    "    eval_indices = unsampled_indices[total_unsampled//2:]\n",
    "\n",
    "    # Store the posterior in a num_nodes-by-num_classes matrix.\n",
    "    # Posteriors in non-leaf cells will be zero everywhere\n",
    "    # and later changed to uniform.\n",
    "    node_counts = tree.tree_.n_node_samples\n",
    "    class_counts = np.zeros((len(node_counts), K))\n",
    "    est_nodes = tree.apply(X[vote_indices])\n",
    "    est_classes = y[vote_indices]\n",
    "    for i in range(len(est_nodes)):\n",
    "        class_counts[est_nodes[i], est_classes[i]] += 1\n",
    "\n",
    "    # Total number of estimation points in each leaf.\n",
    "    row_sums = class_counts.sum(axis=1)\n",
    "    row_sums[row_sums == 0] = 1  # Avoid divide by zero.\n",
    "    class_probs = class_counts / row_sums[:, None]\n",
    "\n",
    "    # Make the nodes that have no estimation indices uniform.\n",
    "    # This includes non-leaf nodes, but tha t will not affect the estimate.\n",
    "    class_probs[np.argwhere(class_probs.sum(axis=1) == 0)] = [1 / K]*K\n",
    "    # Apply finite sample correction and renormalize.\n",
    "    where_0 = np.argwhere(class_probs == 0)\n",
    "    for elem in where_0:\n",
    "        class_probs[elem[0], elem[1]] = 1 / \\\n",
    "            (kappa*class_counts.sum(axis=1)[elem[0]])\n",
    "    row_sums = class_probs.sum(axis=1)\n",
    "    class_probs = class_probs / row_sums[:, None]\n",
    "\n",
    "    # Place evaluation points in their corresponding leaf node.\n",
    "    # Store evaluation posterior in a num_eval-by-num_class matrix.\n",
    "    eval_class_probs = class_probs[tree.apply(X[eval_indices])]\n",
    "    # eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "    eval_entropies = [entropy(posterior, base=base)\n",
    "                      for posterior in eval_class_probs]\n",
    "    return np.mean(eval_entropies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uf(X, y, n_estimators=300, max_samples=.4, base=2, kappa=3):\n",
    "    # Build forest with default parameters.\n",
    "    model = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                              n_estimators=n_estimators,\n",
    "                              max_samples=max_samples,\n",
    "                              n_jobs=40,\n",
    "                              bootstrap=False)\n",
    "    model.fit(X, y)\n",
    "    n = X.shape[0]\n",
    "    K = model.n_classes_\n",
    "    _, y = np.unique(y, return_inverse=True)\n",
    "\n",
    "    cond_entropy = 0\n",
    "    final_null_dist = [0] * 100\n",
    "\n",
    "    # Get real test statistics\n",
    "    for tree_idx, tree in enumerate(model):\n",
    "        # Find the indices of the training set used for partition.\n",
    "        sampled_indices = model.estimators_samples_[tree_idx]\n",
    "        unsampled_indices = np.delete(np.arange(0, n), sampled_indices)\n",
    "        np.random.shuffle(unsampled_indices)\n",
    "\n",
    "        cond_entropy += test_stat_helper(tree, tree_idx,\n",
    "                                         X, y, sampled_indices, unsampled_indices, K)\n",
    "\n",
    "        for j in range(100):\n",
    "            np.random.seed(j)  # get same shuffle across\n",
    "            null_unsampled_indices = unsampled_indices\n",
    "            np.random.shuffle(null_unsampled_indices)\n",
    "            final_null_dist[j] += test_stat_helper(\n",
    "                tree, tree_idx, X, y, sampled_indices, null_unsampled_indices, K)\n",
    "\n",
    "    new_final_null_dist = [entropy([np.mean(\n",
    "        y), 1 - np.mean(y)], base=2) - val / n_estimators for val in final_null_dist]\n",
    "\n",
    "    final_stat = entropy([np.mean(y), 1 - np.mean(y)],\n",
    "                         base=2) - cond_entropy / n_estimators\n",
    "    return final_stat, new_final_null_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLE_SIZE = 100\n",
    "STEP_SIZE = 20\n",
    "SAMP_SIZES = range(10, MAX_SAMPLE_SIZE + STEP_SIZE, STEP_SIZE)\n",
    "POWER_REPS = 20\n",
    "\n",
    "SIMULATIONS = [\n",
    "    #\"linear\": \"Linear\",\n",
    "    #\"multimodal_independence\": \"Independence\"\n",
    "    #linear, \n",
    "    multimodal_independence\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [01:08<21:50, 68.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.49\n",
      "Test stat: 0.15098250045277772\n",
      "Null dist: [0.13246716628897526, 0.16913473683127644, 0.1299241912973622, 0.21112465016096238, 0.1550214701164534]\n"
     ]
    }
   ],
   "source": [
    "def estimate_power(sim):\n",
    "    samp_size_dict = dict()\n",
    "    samp_size_dict['sample_sizes'] = SAMP_SIZES\n",
    "    samp_size_dict['n_power_reps'] = POWER_REPS\n",
    "    power = []\n",
    "    for n_samples in SAMP_SIZES: \n",
    "        pvalues = []\n",
    "        samp_size_dict[n_samples] = {'stats': [], 'null_dists': []}\n",
    "        for p in tqdm(range(POWER_REPS)):\n",
    "            np.random.seed(None)\n",
    "            matrix1, matrix2 = multimodal_independence(n_samples, 2)\n",
    "            x, y = k_sample_transform([matrix1, matrix2])\n",
    "            stat, null_dist = uf(x, y.ravel())\n",
    "            samp_size_dict[n_samples]['stats'].append(stat)\n",
    "            samp_size_dict[n_samples]['null_dists'].append(null_dist)\n",
    "            pvalue = np.mean(np.asarray(null_dist) >= stat)\n",
    "            print(\"P-value: \" + str(pvalue))\n",
    "            print(f'Test stat: {stat}')\n",
    "            print(f'Null dist: {null_dist[:5]}')\n",
    "            pvalues.append(pvalue)\n",
    "        for pval in pvalues: \n",
    "            if pval <= 0.05: \n",
    "                num=num+1; \n",
    "        power.append(num / POWER_REPS) \n",
    "        #power.append((pvalues >= 0.05).sum() / POWER_REPS)\n",
    "        \n",
    "    with open('C:/Users/siptest/Desktop/NDD/multimodal_independence_power_reps_honest.pkl', 'wb') as handle: \n",
    "        pickle.dump(samp_size_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    np.savetxt('C:/Users/siptest/Desktop/NDD/multimodal_independence_power_honest.csv', power, delimiter=',')\n",
    "    \n",
    "    return power\n",
    "\n",
    "# matrix1, matrix2 = multimodal_independence(10, 1)\n",
    "# matrix1, matrix2 = indep_sim('multimodal_independence', 10, 1)\n",
    "# x, y = k_sample_transform([matrix1, matrix2])\n",
    "# print(matrix1.shape, y.shape)\n",
    "estimate_power(SIMULATIONS[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
