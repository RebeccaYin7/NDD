{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siptest\\anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\siptest\\anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\siptest\\anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\siptest\\anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "C:\\Users\\siptest\\anaconda3\\envs\\py36\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import hyppo\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "from hyppo.independence.base import IndependenceTest\n",
    "from hyppo._utils import perm_test\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import copy \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree._classes import DecisionTreeClassifier\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import entropy, multivariate_normal\n",
    "from scipy.integrate import nquad\n",
    "from hyppo.sims import ksample_sim\n",
    "from hyppo.ksample._utils import k_sample_transform\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from hyppo.sims import *\n",
    "from hyppo.ksample._utils import k_sample_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarks import power_2samp_sample\n",
    "from hyppo.independence import CCA, Dcorr, HHG, Hsic, RV, MGC\n",
    "from hyppo.sims import *\n",
    "\n",
    "import sys\n",
    "sys.executable\n",
    "sys.path\n",
    "#sys.path.append('C:\\\\Users\\\\siptest\\\\AppData\\\\Roaming\\\\Python\\\\Python36\\\\Scripts')\n",
    "#sys.path.append('C:\\\\Users\\\\siptest\\\\Desktop\\\\R-3.6.2\\\\bin\\\\x64')\n",
    "\n",
    "import sys, os\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(color_codes=True, style='white', context='talk', font_scale=2)\n",
    "PALETTE = sns.color_palette(\"Set1\")\n",
    "sns.set_palette(PALETTE[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stat_helper(tree, tree_idx, X, y, sampled_indices, unsampled_indices, K, kappa = 3, base = 2): \n",
    "        # Randomly split the rest into voting and evaluation.\n",
    "            \n",
    "        total_unsampled = len(unsampled_indices)\n",
    "        #np.random.shuffle(unsampled_indices)\n",
    "        vote_indices = unsampled_indices[:total_unsampled//2]\n",
    "        eval_indices = unsampled_indices[total_unsampled//2:]\n",
    "            \n",
    "        # Store the posterior in a num_nodes-by-num_classes matrix.\n",
    "        # Posteriors in non-leaf cells will be zero everywhere\n",
    "        # and later changed to uniform.\n",
    "        node_counts = tree.tree_.n_node_samples\n",
    "        class_counts = np.zeros((len(node_counts), K))\n",
    "        est_nodes = tree.apply(X[vote_indices])\n",
    "        est_classes = y[vote_indices]\n",
    "        for i in range(len(est_nodes)):\n",
    "            class_counts[est_nodes[i], est_classes[i]] += 1\n",
    "\n",
    "        row_sums = class_counts.sum(axis=1) # Total number of estimation points in each leaf.\n",
    "        row_sums[row_sums == 0] = 1 # Avoid divide by zero.\n",
    "        class_probs = class_counts / row_sums[:, None]\n",
    "\n",
    "        # Make the nodes that have no estimation indices uniform.\n",
    "        # This includes non-leaf nodes, but tha t will not affect the estimate.\n",
    "        class_probs[np.argwhere(class_probs.sum(axis = 1) == 0)] = [1 / K]*K\n",
    "        # Apply finite sample correction and renormalize.\n",
    "        where_0 = np.argwhere(class_probs == 0)\n",
    "        for elem in where_0:\n",
    "            class_probs[elem[0], elem[1]] = 1 / (kappa*class_counts.sum(axis = 1)[elem[0]])\n",
    "        row_sums = class_probs.sum(axis=1)\n",
    "        class_probs = class_probs / row_sums[:, None]\n",
    "\n",
    "        # Place evaluation points in their corresponding leaf node.\n",
    "        # Store evaluation posterior in a num_eval-by-num_class matrix.\n",
    "        eval_class_probs = class_probs[tree.apply(X[eval_indices])]\n",
    "        # eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "        eval_entropies = [entropy(posterior, base = base) for posterior in eval_class_probs]\n",
    "        return np.mean(eval_entropies)\n",
    "        #return cond_entropy\n",
    "        \n",
    "#def uf(self, X, y, n_estimators = 300, max_samples = .4, base = np.exp(1), kappa = 3):\n",
    "def uf(X, y, n_estimators = 300, max_samples = .4, base = 2, kappa = 3):    \n",
    "    # Build forest with default parameters.\n",
    "    model = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                              n_estimators=n_estimators, \n",
    "                              max_samples=max_samples, \n",
    "                              bootstrap=False)\n",
    "    model.fit(X, y)\n",
    "    n = X.shape[0]\n",
    "    K = model.n_classes_\n",
    "    _, y = np.unique(y, return_inverse=True)\n",
    "\n",
    "    cond_entropy = 0\n",
    "    final_null_dist = [0] * 100 \n",
    "\n",
    "    for tree_idx, tree in enumerate(model):\n",
    "        # Find the indices of the training set used for partition.\n",
    "        sampled_indices = model.estimators_samples_[tree_idx]\n",
    "        unsampled_indices = np.delete(np.arange(0,n), sampled_indices)\n",
    "        np.random.shuffle(unsampled_indices)\n",
    "\n",
    "        #return entropy([np.mean(y), 1 - np.mean(y)], base = 2) - cond_entropy / n_estimators\n",
    "        cond_entropy += test_stat_helper(tree, tree_idx, X, y, sampled_indices, unsampled_indices, K)\n",
    "\n",
    "\n",
    "        #tree_null_stats = []\n",
    "        #null_stat = 0\n",
    "        #null_y = y[unsampled_indices]\n",
    "        #np.random.shuffle(null_y)\n",
    "        # 100 is reps - since we didn't pass in \n",
    "        for j in range(100): \n",
    "            #np.random.shuffle(y[unsampled_indices])\n",
    "            #null_y = y[unsampled_indices]\n",
    "            np.random.seed(j) # get same shuffle across\n",
    "            null_unsampled_indices = unsampled_indices\n",
    "            np.random.shuffle(null_unsampled_indices)\n",
    "            #null_stat = self.test_stat_helper(tree, tree_idx, X, y[unsampled_indices], sampled_indices, unsampled_indices, null_stat, K)\n",
    "            final_null_dist[j] += test_stat_helper(tree, tree_idx, X, y, sampled_indices, null_unsampled_indices, K)\n",
    "            #mi_null_stat = entropy([np.mean(y), 1 - np.mean(y)], base = 2) - null_stat / n_estimators\n",
    "            #tree_null_stats.append(null_stat)\n",
    "        #null_stat = entropy([np.mean(y[unsampled_indices]), 1 - np.mean(y[unsampled_indices])], base = 2) - null_stat / n_estimators\n",
    "        #null_dist.append(tree_null_stats)\n",
    "        #final_null_dist.append(null_stat) \n",
    "\n",
    "    #final_stat = entropy([np.mean(y), 1 - np.mean(y)], base = 2) - cond_entropy / n_estimators\n",
    "\n",
    "\n",
    "    new_final_null_dist = [entropy([np.mean(y), 1 - np.mean(y)], base = 2) - val / n_estimators for val in final_null_dist]\n",
    "\n",
    "    #print(entropy([np.mean(y), 1 - np.mean(y)], base = 2))\n",
    "    #print(entropy([np.mean(y), 1 - np.mean(y)], base = 2) - cond_entropy / n_estimators)\n",
    "    final_stat = entropy([np.mean(y), 1 - np.mean(y)], base = 2) - cond_entropy / n_estimators\n",
    "    return final_stat, new_final_null_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"class UF(IndependenceTest): \n",
    "    def _init_(self, compute_distance='euclidean', bias = False, **kwargs): \n",
    "        self._name_= \"UF\"\n",
    "        IndepedenceTest._init_(self)\n",
    "        \n",
    "    def test_stat_helper(self, tree, tree_idx, X, y, sampled_indices, unsampled_indices, K, kappa = 3, base = 2): \n",
    "        # Randomly split the rest into voting and evaluation.\n",
    "            \n",
    "            total_unsampled = len(unsampled_indices)\n",
    "            #np.random.shuffle(unsampled_indices)\n",
    "            vote_indices = unsampled_indices[:total_unsampled//2]\n",
    "            eval_indices = unsampled_indices[total_unsampled//2:]\n",
    "            \n",
    "            # Store the posterior in a num_nodes-by-num_classes matrix.\n",
    "            # Posteriors in non-leaf cells will be zero everywhere\n",
    "            # and later changed to uniform.\n",
    "            node_counts = tree.tree_.n_node_samples\n",
    "            class_counts = np.zeros((len(node_counts), K))\n",
    "            est_nodes = tree.apply(X[vote_indices])\n",
    "            est_classes = y[vote_indices]\n",
    "            for i in range(len(est_nodes)):\n",
    "                class_counts[est_nodes[i], est_classes[i]] += 1\n",
    "            \n",
    "            row_sums = class_counts.sum(axis=1) # Total number of estimation points in each leaf.\n",
    "            row_sums[row_sums == 0] = 1 # Avoid divide by zero.\n",
    "            class_probs = class_counts / row_sums[:, None]\n",
    "            \n",
    "            # Make the nodes that have no estimation indices uniform.\n",
    "            # This includes non-leaf nodes, but tha t will not affect the estimate.\n",
    "            class_probs[np.argwhere(class_probs.sum(axis = 1) == 0)] = [1 / K]*K\n",
    "            # Apply finite sample correction and renormalize.\n",
    "            where_0 = np.argwhere(class_probs == 0)\n",
    "            for elem in where_0:\n",
    "                class_probs[elem[0], elem[1]] = 1 / (kappa*class_counts.sum(axis = 1)[elem[0]])\n",
    "            row_sums = class_probs.sum(axis=1)\n",
    "            class_probs = class_probs / row_sums[:, None]\n",
    "            \n",
    "            # Place evaluation points in their corresponding leaf node.\n",
    "            # Store evaluation posterior in a num_eval-by-num_class matrix.\n",
    "            eval_class_probs = class_probs[tree.apply(X[eval_indices])]\n",
    "            # eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "            eval_entropies = [entropy(posterior, base = base) for posterior in eval_class_probs]\n",
    "            return np.mean(eval_entropies)\n",
    "            #return cond_entropy\n",
    "        \n",
    "    #def uf(self, X, y, n_estimators = 300, max_samples = .4, base = np.exp(1), kappa = 3):\n",
    "    def uf(self, X, y, n_estimators = 300, max_samples = .4, base = 2, kappa = 3):    \n",
    "        # Build forest with default parameters.\n",
    "        model = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                                  n_estimators=n_estimators, \n",
    "                                  max_samples=max_samples, \n",
    "                                  bootstrap=False)\n",
    "        model.fit(X, y)\n",
    "        n = X.shape[0]\n",
    "        K = model.n_classes_\n",
    "        _, y = np.unique(y, return_inverse=True)\n",
    "        \n",
    "        cond_entropy = 0\n",
    "        final_null_dist = [0] * 100 \n",
    "    \n",
    "        for tree_idx, tree in enumerate(model):\n",
    "            # Find the indices of the training set used for partition.\n",
    "            sampled_indices = model.estimators_samples_[tree_idx]\n",
    "            unsampled_indices = np.delete(np.arange(0,n), sampled_indices)\n",
    "            np.random.shuffle(unsampled_indices)\n",
    "\n",
    "            #return entropy([np.mean(y), 1 - np.mean(y)], base = 2) - cond_entropy / n_estimators\n",
    "            cond_entropy += self.test_stat_helper(tree, tree_idx, X, y, sampled_indices, unsampled_indices, K)\n",
    "            \n",
    "        \n",
    "            #tree_null_stats = []\n",
    "            #null_stat = 0\n",
    "            #null_y = y[unsampled_indices]\n",
    "            #np.random.shuffle(null_y)\n",
    "            # 100 is reps - since we didn't pass in \n",
    "            for j in range(100): \n",
    "                #np.random.shuffle(y[unsampled_indices])\n",
    "                #null_y = y[unsampled_indices]\n",
    "                np.random.seed(j) # get same shuffle across\n",
    "                null_unsampled_indices = unsampled_indices\n",
    "                np.random.shuffle(null_unsampled_indices)\n",
    "                #null_stat = self.test_stat_helper(tree, tree_idx, X, y[unsampled_indices], sampled_indices, unsampled_indices, null_stat, K)\n",
    "                final_null_dist[j] += self.test_stat_helper(tree, tree_idx, X, y, sampled_indices, null_unsampled_indices, K)\n",
    "                #mi_null_stat = entropy([np.mean(y), 1 - np.mean(y)], base = 2) - null_stat / n_estimators\n",
    "                #tree_null_stats.append(null_stat)\n",
    "            #null_stat = entropy([np.mean(y[unsampled_indices]), 1 - np.mean(y[unsampled_indices])], base = 2) - null_stat / n_estimators\n",
    "            #null_dist.append(tree_null_stats)\n",
    "            #final_null_dist.append(null_stat) \n",
    "            \n",
    "        #final_stat = entropy([np.mean(y), 1 - np.mean(y)], base = 2) - cond_entropy / n_estimators\n",
    "        \n",
    "        \n",
    "        new_final_null_dist = [entropy([np.mean(y), 1 - np.mean(y)], base = 2) - val / n_estimators for val in final_null_dist]\n",
    "         \n",
    "        #print(entropy([np.mean(y), 1 - np.mean(y)], base = 2))\n",
    "        #print(entropy([np.mean(y), 1 - np.mean(y)], base = 2) - cond_entropy / n_estimators)\n",
    "        final_stat = entropy([np.mean(y), 1 - np.mean(y)], base = 2) - cond_entropy / n_estimators\n",
    "        return final_stat, new_final_null_dist\n",
    "    \n",
    "    def _statistic(self, X, y): \n",
    "        stat, null_dist = self.uf(X, y)\n",
    "        self.stat = stat\n",
    "        num = 0\n",
    "        for val in null_dist: \n",
    "            if val <= stat: \n",
    "                num = num + 1\n",
    "        pvalue = num / 100 \n",
    "        #pvalue = (null_dist <= self.stat).sum() / 100     \n",
    "        return stat, pvalue\n",
    "    #def test(self, X, y): \n",
    "        #return self.perm_test(X, y)\n",
    "    def test(self, X, y, reps = 100, workers = -1):\n",
    "        #stat, pvalue, _ = perm_test(self._statistic, X, y, reps = reps, is_distsim=True)\n",
    "        stat, pvalue = self._statistic(X, y)\n",
    "        self.stat = stat\n",
    "        self.pvalue = pvalue\n",
    "        \n",
    "        return stat, pvalue\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLE_SIZE = 100\n",
    "STEP_SIZE = 20\n",
    "SAMP_SIZES = range(10, MAX_SAMPLE_SIZE + STEP_SIZE, STEP_SIZE)\n",
    "#POWER_REPS = 5\n",
    "POWER_REPS = 20\n",
    "\n",
    "SIMULATIONS = [\n",
    "    #\"linear\": \"Linear\",\n",
    "    #\"multimodal_independence\": \"Independence\"\n",
    "    #linear, \n",
    "    multimodal_independence\n",
    "]\n",
    "\n",
    "#TESTS = [\n",
    "    #UF, \n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_power(sim):\n",
    "    #est_power = np.array([np.mean([power_2samp_sample(test, trans_2samp, sim, n=i) for _ in range(POWER_REPS)])\n",
    "                          #for i in SAMP_SIZES])\n",
    "    \"\"\"est_power = []\n",
    "    for i in SAMP_SIZES: \n",
    "        power = np.mean([power_2samp_sample(test, trans_2samp, sim, n=i, p = 1, reps = 100, workers = 1) for _ in range(POWER_REPS)])\n",
    "        est_power.append(power)\n",
    "        np.savetxt('C:/Users/siptest/Desktop/NDD/{}_{}1D_HonestSampling.csv'.format(sim.__name__, \"UF\"),\n",
    "               est_power, delimiter=',')\"\"\"\n",
    "    power = []\n",
    "\n",
    "    samp_size_dict = dict()\n",
    "    for x in SAMP_SIZES: \n",
    "        pvalues = []\n",
    "        num = 0\n",
    "        numNull = 0\n",
    "        stats = []\n",
    "        null_dists = []\n",
    "        for p in tqdm(range(POWER_REPS)): \n",
    "            print(\"Power Rep: \" + str(p))\n",
    "            #matrix1, matrix2 = ksample_sim.trans_2samp(multimodal_independence, 100, 1, noise = False, trans = 0.3)\n",
    "            matrix1, matrix2 = ksample_sim.trans_2samp(multimodal_independence, 100, 1, noise = True, trans = 0.3)\n",
    "\n",
    "            x, y = k_sample_transform([matrix1, matrix2])\n",
    "            stat, nullDist = uf(x, y)\n",
    "            stats.append(stat)\n",
    "            null_dists.append(nullDist)\n",
    "            print(\"Test Stat: \" + str(stat))\n",
    "            pvalue = np.mean(np.asarray(nullDist) >= stat)\n",
    "            #pvalue = (nullDist <= ce).sum() / 500\n",
    "            print(\"P-value: \" + str(pvalue))\n",
    "            pvalues.append(pvalue)\n",
    "        samp_size_dict[x] = {'stats': stats, 'nullDists': null_dists}\n",
    "        for pval in pvalues: \n",
    "            if pval <= 0.05: \n",
    "                num=num+1; \n",
    "        power.append(num / POWER_REPS) \n",
    "        #power.append((pvalues >= 0.05).sum() / POWER_REPS)\n",
    "        \n",
    "    with open('honest_sampling_ind.pickle', 'wb') as handle: \n",
    "        pickle.dump(samp_size_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('honest_sampling_ind.pickle', 'rb') as handle: \n",
    "        b = pickle.load(handle)\n",
    "    print(samp_size_dict == b)\n",
    "\n",
    "    np.savetxt('C:/Users/siptest/Desktop/hyppo/benchmarks/2samp_vs_samplesize/UF_HonestSampling_Ind.csv',\n",
    "                   power, delimiter=',')\n",
    "        \n",
    "    \n",
    "    return est_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "outputs = Parallel(n_jobs=-1, verbose=100)(\n",
    "    [delayed(estimate_power)(sim) for sim in SIMULATIONS]\n",
    ")\n",
    "#estimate_power(sim, test) for sim in SIMULATIONS for test in TESTS\n",
    "#outputs = []\n",
    "#for test in TESTS: \n",
    "    #for sim in SIMULATIONS:\n",
    "        #estimate_power(sim, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_power2(): \n",
    "    \n",
    "    sim_title = [\n",
    "        #\"Linear\", \n",
    "        \"Independence\"\n",
    "    ]\n",
    "    sim = multimodal_independence\n",
    "    power = np.genfromtxt('C:/Users/siptest/Desktop/NDD/{}_{}3D5Power.csv'.format(sim.__name__, \"UF\"),\n",
    "                                      delimiter=',')\n",
    "    plt.plot(power)\n",
    "    plt.yticks([0, 1])\n",
    "    plt.axhline(y=0.05, color='b', linestyle='--')\n",
    "    positions = (0, 3, 5)\n",
    "    labels = (\"5\", \"65\", \"105\")\n",
    "    plt.xticks(positions, labels)\n",
    "    plt.xlabel(\"Sample Size\")\n",
    "    plt.ylabel(\"Mean Power from 5 Reps\")\n",
    "    plt.savefig(\"C:/Users/siptest/Desktop/NDD/Independence_UF_3D5PowerFig.jpg\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_power(): \n",
    "    fig, ax = plt.subplots(nrows = 1, ncols =1, figsize = (28, 24))\n",
    "    \n",
    "    sim_title = [\n",
    "        #\"Linear\", \n",
    "        \"Independence\"\n",
    "    ]\n",
    "        \n",
    "    for i, col in enumerate(ax):\n",
    "        sim = SIMULATIONS[i]\n",
    "        for test in TESTS:\n",
    "                power = np.genfromtxt('C:/Users/siptest/Desktop/NDD/{}_{}3D5Power.csv'.format(sim.__name__, \"UF\"),\n",
    "                                      delimiter=',')\n",
    "        col.plot(SAMP_SIZES, power, label=\"UF\", lw=1)\n",
    "        col.set_xticks([])\n",
    "        col.set_yticks([0, 1])\n",
    "        col.set_title(sim_title[i])\n",
    "        if sim == multimodal_independence: \n",
    "            plt.axhline(y=0.05, color='b', linestyle='--')\n",
    "\n",
    "    fig.text(0.5, 0.08, 'Sample Size', ha='center')\n",
    "    fig.text(0.07, 0.5, 'Statistical Power', va='center', rotation='vertical')\n",
    "    leg = plt.legend(bbox_to_anchor=(0.5, 0.08), bbox_transform=plt.gcf().transFigure,\n",
    "                     ncol=5, loc='upper center')\n",
    "    leg.get_frame().set_linewidth(0.0)\n",
    "    for legobj in leg.legendHandles:\n",
    "        legobj.set_linewidth(5.0)\n",
    "    plt.subplots_adjust(hspace=.50)\n",
    "    plt.savefig('C:/Users/siptest/Desktop/NDD/2samp_power_indep3D5Power.png', transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_power2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
