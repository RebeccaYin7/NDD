{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import proglearn\n",
    "#from proglearn.forest import UncertaintyForest\n",
    "import hyppo\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "from hyppo.independence.base import IndependenceTest\n",
    "from hyppo._utils import perm_test\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import copy \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree._classes import DecisionTreeClassifier\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import entropy, multivariate_normal\n",
    "from scipy.integrate import nquad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "sys.path\n",
    "sys.path.append('C:\\\\Users\\\\siptest\\\\AppData\\\\Roaming\\\\Python\\\\Python36\\\\Scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uf(X, y, n_estimators = 300, max_samples = .4, base = np.exp(1), kappa = 3):\n",
    "    \n",
    "    # Build forest with default parameters.\n",
    "    model = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                              n_estimators=n_estimators, \n",
    "                              max_samples=max_samples, \n",
    "                              bootstrap=False)\n",
    "    model.fit(X, y)\n",
    "    n = X.shape[0]\n",
    "    K = model.n_classes_\n",
    "    _, y = np.unique(y, return_inverse=True)\n",
    "    \n",
    "    cond_entropy = 0\n",
    "    for tree_idx, tree in enumerate(model):\n",
    "        # Find the indices of the training set used for partition.\n",
    "        sampled_indices = model.estimators_samples_[tree_idx]\n",
    "        unsampled_indices = np.delete(np.arange(0,n), sampled_indices)\n",
    "        \n",
    "        # Randomly split the rest into voting and evaluation.\n",
    "        total_unsampled = len(unsampled_indices)\n",
    "        np.random.shuffle(unsampled_indices)\n",
    "        vote_indices = unsampled_indices[:total_unsampled//2]\n",
    "        eval_indices = unsampled_indices[total_unsampled//2:]\n",
    "        \n",
    "        # Store the posterior in a num_nodes-by-num_classes matrix.\n",
    "        # Posteriors in non-leaf cells will be zero everywhere\n",
    "        # and later changed to uniform.\n",
    "        node_counts = tree.tree_.n_node_samples\n",
    "        class_counts = np.zeros((len(node_counts), K))\n",
    "        est_nodes = tree.apply(X[vote_indices])\n",
    "        est_classes = y[vote_indices]\n",
    "        for i in range(len(est_nodes)):\n",
    "            class_counts[est_nodes[i], est_classes[i]] += 1\n",
    "        \n",
    "        row_sums = class_counts.sum(axis=1) # Total number of estimation points in each leaf.\n",
    "        row_sums[row_sums == 0] = 1 # Avoid divide by zero.\n",
    "        class_probs = class_counts / row_sums[:, None]\n",
    "        \n",
    "        # Make the nodes that have no estimation indices uniform.\n",
    "        # This includes non-leaf nodes, but that will not affect the estimate.\n",
    "        class_probs[np.argwhere(class_probs.sum(axis = 1) == 0)] = [1 / K]*K\n",
    "        \n",
    "        # Apply finite sample correction and renormalize.\n",
    "        where_0 = np.argwhere(class_probs == 0)\n",
    "        for elem in where_0:\n",
    "            class_probs[elem[0], elem[1]] = 1 / (kappa*class_counts.sum(axis = 1)[elem[0]])\n",
    "        row_sums = class_probs.sum(axis=1)\n",
    "        class_probs = class_probs / row_sums[:, None]\n",
    "        \n",
    "        # Place evaluation points in their corresponding leaf node.\n",
    "        # Store evaluation posterior in a num_eval-by-num_class matrix.\n",
    "        eval_class_probs = class_probs[tree.apply(X[eval_indices])]\n",
    "        # eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "        eval_entropies = [entropy(posterior) for posterior in eval_class_probs]\n",
    "        cond_entropy += np.mean(eval_entropies)\n",
    "\n",
    "      \n",
    "    return cond_entropy / n_estimators\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data2(n, d, mu = 1):\n",
    "    n_1 = np.random.binomial(n, .5) # number of class 1\n",
    "    mean = np.zeros(d)\n",
    "    mean[0] = mu\n",
    "    X_1 = np.random.multivariate_normal(mean, np.eye(d), n_1)\n",
    "    \n",
    "    X = np.concatenate((X_1, np.random.multivariate_normal(-mean, np.eye(d), n - n_1)))\n",
    "    y = np.concatenate((np.repeat(1, n_1), np.repeat(0, n - n_1)))\n",
    "  \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, d, mu = 1, var1 = 1, pi = 0.5, three_class = False):\n",
    "    \n",
    "    means, Sigmas, probs = _make_params(d, mu = mu, var1 = var1, pi = pi, three_class = three_class)\n",
    "    counts = np.random.multinomial(n, probs, size = 1)[0]\n",
    "    \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    for k in range(len(probs)):\n",
    "        X_data.append(np.random.multivariate_normal(means[k], Sigmas[k], counts[k]))\n",
    "        y_data.append(np.repeat(k, counts[k]))\n",
    "    X = np.concatenate(tuple(X_data))\n",
    "    y = np.concatenate(tuple(y_data))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_params(d, mu = 1, var1 = 1, pi = 0.5, three_class = False):\n",
    "    \n",
    "    if three_class:\n",
    "        return _make_three_class_params(d, mu, pi)\n",
    "    \n",
    "    mean = np.zeros(d)\n",
    "    mean[0] = mu\n",
    "    means = [mean, -mean]\n",
    "\n",
    "    Sigma1 = np.eye(d)\n",
    "    Sigma1[0, 0] = var1\n",
    "    Sigmas = [np.eye(d), Sigma1]\n",
    "    \n",
    "    probs = [pi, 1 - pi]\n",
    "    \n",
    "    return means, Sigmas, probs\n",
    "\n",
    "def _make_three_class_params(d, mu, pi):\n",
    "    \n",
    "    means = []\n",
    "    mean = np.zeros(d)\n",
    "    \n",
    "    mean[0] = mu\n",
    "    means.append(copy.deepcopy(mean))\n",
    "    \n",
    "    mean[0] = -mu\n",
    "    means.append(copy.deepcopy(mean))\n",
    "    \n",
    "    mean[0] = 0\n",
    "    mean[d-1] = mu\n",
    "    means.append(copy.deepcopy(mean))\n",
    "    \n",
    "    Sigmas = [np.eye(d)]*3\n",
    "    probs = [pi, (1 - pi) / 2, (1 - pi) / 2]\n",
    "    \n",
    "    return means, Sigmas, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mutual_info(d, base = np.exp(1), mu = 1, var1 = 1, pi = 0.5, three_class = False):\n",
    "    \n",
    "    if d > 1:\n",
    "        dim = 2\n",
    "    else:\n",
    "        dim = 1\n",
    " \n",
    "    means, Sigmas, probs = _make_params(dim, mu = mu, var1 = var1, pi = pi, three_class = three_class)\n",
    "    \n",
    "    # Compute entropy and X and Y.\n",
    "    def func(*args):\n",
    "        x = np.array(args)\n",
    "        p = 0\n",
    "        for k in range(len(means)):\n",
    "            p += probs[k] * multivariate_normal.pdf(x, means[k], Sigmas[k])\n",
    "        return -p * np.log(p) / np.log(base)\n",
    "\n",
    "    scale = 10\n",
    "    lims = [[-scale, scale]]*dim\n",
    "    H_X, int_err = nquad(func, lims)\n",
    "    H_Y = entropy(probs, base = base)\n",
    "    \n",
    "    # Compute MI.\n",
    "    H_XY = 0\n",
    "    for k in range(len(means)):\n",
    "        H_XY += probs[k] * (dim * np.log(2*np.pi) + np.log(np.linalg.det(Sigmas[k])) + dim) / (2 * np.log(base))\n",
    "    I_XY = H_X - H_XY\n",
    "    \n",
    "    return I_XY, H_X, H_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20 \n",
    "#n = 6000\n",
    "mus = range(5)\n",
    "ds = range(1, 16)\n",
    "mu = 1\n",
    "num_trials = 10\n",
    "#reps = 1\n",
    "d = 2\n",
    "pis = [0.05 * i for i in range(1, 20)]\n",
    "\n",
    "def estimate_mi(X, y, est_H_Y, norm_factor): \n",
    "    return (est_H_Y - uf(np.array(X), y)) / norm_factor\n",
    "\n",
    "def mi(X, y, n, d, pis, num_trials):\n",
    "    #def worker(t): \n",
    "        #X, y = generate_data(n, d, pi = elem)\n",
    "        \n",
    "        #I_XY, H_X, H_Y = compute_mutual_info(d, pi = elem)\n",
    "        I_XY, H_X, H_Y = compute_mutual_info(d)\n",
    "        norm_factor = min(H_X, H_Y)\n",
    "        \n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        est_H_Y = entropy(counts, base=np.exp(1))\n",
    "        ret = []\n",
    "        ret.append(estimate_mi(X, y, est_H_Y, norm_factor))\n",
    "        #return tuple(ret)\n",
    "        return ret[0]\n",
    "    \n",
    "    #output = np.zeros((len(pis), num_trials))\n",
    "    #for i, elem in enumerate(pis): \n",
    "        #results = np.array(Parallel(n_jobs=-2)(delayed(worker)(t) for t in range(num_trials)))\n",
    "        #output[i, :] = results[:, 0]\n",
    "    #return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _perm_stat(X, y, is_distsim = True, permuter = None): \n",
    "    if permuter is None: \n",
    "        order = np.random.permutation(y.shape[0])\n",
    "    else: \n",
    "        order = permuter()\n",
    "    \n",
    "    if is_distsim: \n",
    "        permy = y[order][:, order]\n",
    "    else: \n",
    "        permy = y[order]\n",
    "    \n",
    "    perm_stat = mi(X, permy, n, d, pis, num_trials)\n",
    "    \n",
    "    return perm_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_test(X, y, workers = 1, is_distsim=True, perm_block = None, reps = 5): \n",
    "\n",
    "    # calculate observed test statistic\n",
    "    stat = mi(X, y, n, d, pis, num_trials)\n",
    "    #print(stat) \n",
    "\n",
    "    # calculate null distribution\n",
    "    null_dist = np.array(\n",
    "        Parallel(n_jobs=-2)(\n",
    "            [\n",
    "                delayed(_perm_stat)(X, y, False) \n",
    "                for rep in range(reps)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "            \n",
    "    pvalue = (null_dist >= stat).sum() / reps\n",
    "\n",
    "    # correct for a p-value of 0. This is because, with bootstrapping\n",
    "    # permutations, a p-value of 0 is incorrect\n",
    "    if pvalue == 0:\n",
    "        pvalue = 1 / reps\n",
    "\n",
    "    return stat, pvalue, null_dist\n",
    "    #return pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of reps: 10\n",
      "\n",
      "test statistic, p-value, null distribution\n",
      "(0.34811736574519275, 0.1, array([0.133884  , 0.13853827, 0.14000555, 0.15300503, 0.14349838,\n",
      "       0.1381763 , 0.17116754, 0.17499679, 0.15077395, 0.14111682]))\n",
      "\n",
      "Number of reps: 20\n",
      "\n",
      "test statistic, p-value, null distribution\n"
     ]
    }
   ],
   "source": [
    "#X, y = generate_data(n, d)\n",
    "#print(perm_test(X, y))\n",
    "for i in range(1, 5): \n",
    "    print(\"\\nNumber of reps: \" + str(i*10) + \"\\n\")\n",
    "    print(\"test statistic, p-value, null distribution\")\n",
    "    X, y = generate_data(n, d)\n",
    "    print(perm_test(X, y, reps = i*10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
