{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "toxic-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyppo\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "from hyppo.independence.base import IndependenceTest\n",
    "from hyppo._utils import perm_test\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import copy \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree._classes import DecisionTreeClassifier\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import entropy, multivariate_normal\n",
    "from scipy.integrate import nquad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ethical-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from hyppo.sims import *\n",
    "from hyppo.ksample._utils import k_sample_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "canadian-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarks import power_2samp_sample\n",
    "from hyppo.independence import CCA, Dcorr, HHG, Hsic, RV, MGC\n",
    "from hyppo.sims import *\n",
    "\n",
    "import sys\n",
    "sys.executable\n",
    "sys.path\n",
    "#sys.path.append('C:\\\\Users\\\\siptest\\\\AppData\\\\Roaming\\\\Python\\\\Python36\\\\Scripts')\n",
    "#sys.path.append('C:\\\\Users\\\\siptest\\\\Desktop\\\\R-3.6.2\\\\bin\\\\x64')\n",
    "\n",
    "import sys, os\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "opposed-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(color_codes=True, style='white', context='talk', font_scale=2)\n",
    "PALETTE = sns.color_palette(\"Set1\")\n",
    "sns.set_palette(PALETTE[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "clean-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UF(): \n",
    "    def _init_(self): \n",
    "        self._name_= \"UF\"\n",
    "        \n",
    "    def uf(self, X, y, n_estimators = 300, max_samples = .4, base = np.exp(1), kappa = 3):\n",
    "        \n",
    "        # Build forest with default parameters.\n",
    "        model = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                                  n_estimators=n_estimators, \n",
    "                                  max_samples=max_samples, \n",
    "                                  bootstrap=False)\n",
    "        model.fit(X, y)\n",
    "        n = X.shape[0]\n",
    "        K = model.n_classes_\n",
    "        _, y = np.unique(y, return_inverse=True)\n",
    "        \n",
    "        cond_entropy = 0\n",
    "        for tree_idx, tree in enumerate(model):\n",
    "            # Find the indices of the training set used for partition.\n",
    "            sampled_indices = model.estimators_samples_[tree_idx]\n",
    "            unsampled_indices = np.delete(np.arange(0,n), sampled_indices)\n",
    "            \n",
    "            # Randomly split the rest into voting and evaluation.\n",
    "            total_unsampled = len(unsampled_indices)\n",
    "            np.random.shuffle(unsampled_indices)\n",
    "            vote_indices = unsampled_indices[:total_unsampled//2]\n",
    "            eval_indices = unsampled_indices[total_unsampled//2:]\n",
    "            \n",
    "            # Store the posterior in a num_nodes-by-num_classes matrix.\n",
    "            # Posteriors in non-leaf cells will be zero everywhere\n",
    "            # and later changed to uniform.\n",
    "            node_counts = tree.tree_.n_node_samples\n",
    "            class_counts = np.zeros((len(node_counts), K))\n",
    "            est_nodes = tree.apply(X[vote_indices])\n",
    "            est_classes = y[vote_indices]\n",
    "            for i in range(len(est_nodes)):\n",
    "                class_counts[est_nodes[i], est_classes[i]] += 1\n",
    "            \n",
    "            row_sums = class_counts.sum(axis=1) # Total number of estimation points in each leaf.\n",
    "            row_sums[row_sums == 0] = 1 # Avoid divide by zero.\n",
    "            class_probs = class_counts / row_sums[:, None]\n",
    "            \n",
    "            # Make the nodes that have no estimation indices uniform.\n",
    "            # This includes non-leaf nodes, but tha t will not affect the estimate.\n",
    "            class_probs[np.argwhere(class_probs.sum(axis = 1) == 0)] = [1 / K]*K\n",
    "            \n",
    "            # Apply finite sample correction and renormalize.\n",
    "            where_0 = np.argwhere(class_probs == 0)\n",
    "            for elem in where_0:\n",
    "                class_probs[elem[0], elem[1]] = 1 / (kappa*class_counts.sum(axis = 1)[elem[0]])\n",
    "            row_sums = class_probs.sum(axis=1)\n",
    "            class_probs = class_probs / row_sums[:, None]\n",
    "            \n",
    "            # Place evaluation points in their corresponding leaf node.\n",
    "            # Store evaluation posterior in a num_eval-by-num_class matrix.\n",
    "            eval_class_probs = class_probs[tree.apply(X[eval_indices])]\n",
    "            # eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "            eval_entropies = [entropy(posterior) for posterior in eval_class_probs]\n",
    "            cond_entropy += np.mean(eval_entropies)\n",
    "    \n",
    "          \n",
    "        return cond_entropy / n_estimators\n",
    "   \n",
    "    \"\"\"\"def _perm_stat(self, X, y, is_distsim = True, permuter = None): \n",
    "        if permuter is None: \n",
    "            order = np.random.permutation(y.shape[0])\n",
    "        else: \n",
    "            order = permuter()\n",
    "\n",
    "        if is_distsim: \n",
    "            permy = y[order][:, order]\n",
    "        else: \n",
    "            permy = y[order]\n",
    "\n",
    "        perm_stat = self.uf(X, permy)\n",
    "\n",
    "        return perm_stat\n",
    "    \n",
    "    def perm_test(self, X, y, workers = -1, is_distsim=True, perm_block = None, reps = 100): \n",
    "        # calculate observed test statistic\n",
    "        stat = self.uf(X, y)\n",
    "        #print(stat) \n",
    "\n",
    "        # calculate null distribution\n",
    "        null_dist = np.array(\n",
    "            Parallel(n_jobs=-2)(\n",
    "                [\n",
    "                    delayed(_perm_stat)(X, y, False) \n",
    "                    for rep in range(reps)\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        pvalue = (null_dist >= stat).sum() / reps\n",
    "\n",
    "        # correct for a p-value of 0. This is because, with bootstrapping\n",
    "        # permutations, a p-value of 0 is incorrect\n",
    "        if pvalue == 0:\n",
    "            pvalue = 1 / reps\n",
    "\n",
    "        #return stat, pvalue, null_dist\n",
    "        print(stat, pvalue, null_dist)\n",
    "        return stat, pvalue\"\"\"\n",
    "    \n",
    "    def _statistic(self, X, y): \n",
    "        return self.uf(X, y)\n",
    "    #def test(self, X, y): \n",
    "        #return self.perm_test(X, y)\n",
    "    def test(self, X, y, reps = 100, workers = -1):\n",
    "        stat, pvalue, _ = perm_test(self._statistic, X, y, reps = reps, is_distsim=True)\n",
    "        self.stat = stat\n",
    "        self.pvalue = pvalue\n",
    "        \n",
    "        return stat, pvalue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rapid-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLE_SIZE = 100\n",
    "STEP_SIZE = 5\n",
    "SAMP_SIZES = range(5, MAX_SAMPLE_SIZE + STEP_SIZE, STEP_SIZE)\n",
    "POWER_REPS = 5\n",
    "\n",
    "SIMULATIONS = {\n",
    "    #\"linear\": \"Linear\",\n",
    "    #\"multimodal_independence\": \"Independence\"\n",
    "    linear, \n",
    "    multimodal_independence\n",
    "}\n",
    "\n",
    "TESTS = [\n",
    "    UF, \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exterior-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_power(sim, test):\n",
    "    est_power = np.array([np.mean([power_2samp_sample(test, trans_2samp, sim, n=i) for _ in range(POWER_REPS)])\n",
    "                          for i in SAMP_SIZES])\n",
    "    np.savetxt('C:/Users/Admin/Desktop/{}_{}.csv'.format(sim.__name__, \"UF\"),\n",
    "               est_power, delimiter=',')\n",
    "    \n",
    "    return est_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-hamilton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "outputs = Parallel(n_jobs=-1, verbose=100)(\n",
    "    [delayed(estimate_power)(sim, test) for sim in SIMULATIONS for test in TESTS]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-maryland",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
