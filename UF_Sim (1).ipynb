{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarks import power_2samp_sample\n",
    "from hyppo.independence import CCA, Dcorr, HHG, Hsic, RV, MGC\n",
    "from hyppo.sims import *\n",
    "\n",
    "\n",
    "import sys, os\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyppo\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "from hyppo.independence.base import IndependenceTest\n",
    "from hyppo._utils import perm_test\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import copy \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree._classes import DecisionTreeClassifier\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import entropy, multivariate_normal\n",
    "from scipy.integrate import nquad\n",
    "\n",
    "import sys\n",
    "sys.executable\n",
    "sys.path\n",
    "sys.path.append('C:\\\\Users\\\\siptest\\\\AppData\\\\Roaming\\\\Python\\\\Python36\\\\Scripts')\n",
    "sys.path.append('C:\\\\Users\\\\siptest\\\\Desktop\\\\R-3.6.2\\\\bin\\\\x64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(color_codes=True, style='white', context='talk', font_scale=2)\n",
    "PALETTE = sns.color_palette(\"Set1\")\n",
    "sns.set_palette(PALETTE[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uf(X, y, n_estimators = 300, max_samples = .4, base = np.exp(1), kappa = 3):\n",
    "#def uf(X, y, n_estimators = 300, max_samples = .4, base = 2, kappa = 3):        \n",
    "        # Build forest with default parameters.\n",
    "        model = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                                  n_estimators=n_estimators, \n",
    "                                  max_samples=max_samples, \n",
    "                                  bootstrap=False)\n",
    "        model.fit(X, y)\n",
    "        n = X.shape[0]\n",
    "        K = model.n_classes_\n",
    "        _, y = np.unique(y, return_inverse=True)\n",
    "        \n",
    "        cond_entropy = 0\n",
    "        for tree_idx, tree in enumerate(model):\n",
    "            # Find the indices of the training set used for partition.\n",
    "            sampled_indices = model.estimators_samples_[tree_idx]\n",
    "            unsampled_indices = np.delete(np.arange(0,n), sampled_indices)\n",
    "            \n",
    "            # Randomly split the rest into voting and evaluation.\n",
    "            total_unsampled = len(unsampled_indices)\n",
    "            np.random.shuffle(unsampled_indices)\n",
    "            vote_indices = unsampled_indices[:total_unsampled//2]\n",
    "            eval_indices = unsampled_indices[total_unsampled//2:]\n",
    "            \n",
    "            # Store the posterior in a num_nodes-by-num_classes matrix.\n",
    "            # Posteriors in non-leaf cells will be zero everywhere\n",
    "            # and later changed to uniform.\n",
    "            node_counts = tree.tree_.n_node_samples\n",
    "            class_counts = np.zeros((len(node_counts), K))\n",
    "            est_nodes = tree.apply(X[vote_indices])\n",
    "            est_classes = y[vote_indices]\n",
    "            for i in range(len(est_nodes)):\n",
    "                class_counts[est_nodes[i], est_classes[i]] += 1\n",
    "            \n",
    "            row_sums = class_counts.sum(axis=1) # Total number of estimation points in each leaf.\n",
    "            row_sums[row_sums == 0] = 1 # Avoid divide by zero.\n",
    "            class_probs = class_counts / row_sums[:, None]\n",
    "            \n",
    "            # Make the nodes that have no estimation indices uniform.\n",
    "            # This includes non-leaf nodes, but that will not affect the estimate.\n",
    "            class_probs[np.argwhere(class_probs.sum(axis = 1) == 0)] = [1 / K]*K\n",
    "            \n",
    "            # Apply finite sample correction and renormalize.\n",
    "            where_0 = np.argwhere(class_probs == 0)\n",
    "            for elem in where_0:\n",
    "                class_probs[elem[0], elem[1]] = 1 / (kappa*class_counts.sum(axis = 1)[elem[0]])\n",
    "            row_sums = class_probs.sum(axis=1)\n",
    "            class_probs = class_probs / row_sums[:, None]\n",
    "            \n",
    "            # Place evaluation points in their corresponding leaf node.\n",
    "            # Store evaluation posterior in a num_eval-by-num_class matrix.\n",
    "            eval_class_probs = class_probs[tree.apply(X[eval_indices])]\n",
    "            # eval_class_probs = [class_probs[x] for x in tree.apply(X[eval_indices])]\n",
    "            \n",
    "            #print(eval_class_probs)\n",
    "            \n",
    "            eval_entropies = [entropy(posterior) for posterior in eval_class_probs]\n",
    "            \n",
    "            #print(eval_entropies)\n",
    "            \n",
    "            cond_entropy += np.mean(eval_entropies)\n",
    "    \n",
    "        print(cond_entropy)  \n",
    "        return cond_entropy / n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLE_SIZE = 100\n",
    "#STEP_SIZE = 5\n",
    "STEP_SIZE = 20\n",
    "SAMP_SIZES = range(20, MAX_SAMPLE_SIZE + STEP_SIZE, STEP_SIZE)\n",
    "#SAMP_SIZES = range(5, MAX_SAMPLE_SIZE + STEP_SIZE, STEP_SIZE)\n",
    "POWER_REPS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6931029999092981, 0.6930600580298041, 0.6931066800131775, 0.6931010998521303, 0.6931102001040916, 0.6931119001450408, 0.6931468605599111, 0.6931468605599111, 0.6930751788318458, 0.6926603815637292, 0.6930844592486134, 0.6930971797265786, 0.6931291804519437, 0.6930971797265786, 0.6931346805078614, 0.6931151802186033, 0.6930491773583609, 0.6930026735990573, 0.6931468605599111, 0.6931443005571805, 0.6931383605340142, 0.6931240603817639, 0.6931254004018198, 0.6930930995850211, 0.6930821991524053, 0.6930909995078338, 0.6931471605599452, 0.6931471605599452, 0.6930463571714478, 0.6931315004779901, 0.693141400548809, 0.693137500528711, 0.6931432605548231, 0.6931443005571805, 0.6931455605590705, 0.6931303604656399, 0.6931212603359918, 0.6931383605340142, 0.6930652583228365, 0.6929922925630163, 0.6930751788318458, 0.6928445700343367, 0.6931443005571805, 0.6931462005596252, 0.6931240603817639, 0.693079899050998, 0.6930223553660826, 0.6930677984594171, 0.693079899050998, 0.6931366005226329, 0.6930677984594171, 0.6931471005599432, 0.6931443005571805, 0.6931254004018198, 0.693113560183167, 0.6931102001040916, 0.6931291804519437, 0.693116760251479, 0.6931432605548231, 0.6931267004201329, 0.693062678179701, 0.693113560183167, 0.6931198003100517, 0.6930727587137232, 0.6931391805386118, 0.6931438005561372, 0.6931029999092981, 0.6930909995078338, 0.6931447605579931, 0.6930951596578809, 0.6931447605579931, 0.693062678179701, 0.6931462005596252, 0.693141400548809, 0.6931464605597725, 0.6931267004201329, 0.693104859962931, 0.69301595481978, 0.693104859962931, 0.6931212603359918, 0.6931432605548231, 0.693139960542569, 0.6931010998521303, 0.6930546977088843, 0.693006053920911, 0.6931315004779901, 0.6931315004779901, 0.6931391805386118, 0.6930463571714478, 0.6930126945310066, 0.6931102001040916, 0.6931466805598621, 0.6930346763408155, 0.6930573978729362, 0.6930951596578809, 0.6931066800131775, 0.6931466805598621, 0.6931438005561372, 0.6931471005599432, 0.6931346805078614, 0.6931471005599432, 0.6931438005561372, 0.6931438005561372, 0.6931066800131775, 0.6931336604990145, 0.6931066800131775, 0.693137500528711, 0.6931407005459485, 0.6931432605548231, 0.6929007403150153, 0.6929815514154054, 0.6928639338160292, 0.6930909995078338, 0.6930888594261516, 0.6930909995078338, 0.6931432605548231, 0.6930727587137232, 0.6931420605512071, 0.6930376565613848, 0.6931303604656399, 0.693025495624124, 0.6930126945310066, 0.6930546977088843, 0.6931464605597725, 0.693137500528711, 0.6927826362602763, 0.693137500528711, 0.6931471605599452, 0.6931102001040916, 0.6930930995850211, 0.6931010998521303, 0.6931198003100517, 0.6930971797265786, 0.6931254004018198, 0.6930991597912757, 0.693122680359858, 0.6931459005593992, 0.69301595481978, 0.6931029999092981, 0.693104859962931, 0.693122680359858, 0.6928872380357927, 0.6930346763408155, 0.6931254004018198, 0.6928733755693085, 0.6931471005599432, 0.6931366005226329, 0.6931029999092981, 0.6930844592486134, 0.6930821991524053, 0.6931420605512071, 0.6931447605579931, 0.693116760251479, 0.693141400548809, 0.6931471805599453, 0.6931303604656399, 0.6930652583228365, 0.6931212603359918, 0.6931383605340142, 0.6931240603817639, 0.6930519575374322, 0.6930405967731903, 0.6931468605599111, 0.6931346805078614, 0.6929815514154054, 0.6931291804519437, 0.6931151802186033, 0.6931366005226329, 0.6931326004890856, 0.6929511477499897, 0.6930652583228365, 0.6931470005599345, 0.6931471605599452, 0.6929589887542638, 0.6928194647591654, 0.6931438005561372, 0.6931447605579931, 0.6927986600692193, 0.693122680359858, 0.6931315004779901, 0.6931084600601837, 0.6929224437238923, 0.693031656111244, 0.6931315004779901, 0.693104859962931, 0.6930751788318458, 0.6931464605597725, 0.6930991597912757, 0.6931459005593992, 0.6931471005599432, 0.6930434969764673, 0.6931291804519437, 0.6931267004201329, 0.6931315004779901, 0.6928962795754752, 0.693139960542569, 0.6929051610348317, 0.69301595481978, 0.6930546977088843, 0.693113560183167, 0.6930405967731903, 0.6930844592486134, 0.6929051610348317, 0.6931455605590705, 0.6931471005599432, 0.6929922925630163, 0.6930909995078338, 0.6928346479997213, 0.6931102001040916, 0.693031656111244, 0.6931462005596252, 0.6930821991524053, 0.6931383605340142, 0.6930775589442072, 0.6929431466829601, 0.692882657234926, 0.6930888594261516, 0.6931151802186033, 0.6929138824167064, 0.693113560183167, 0.6930888594261516, 0.6931336604990145, 0.6931212603359918, 0.6931426805531953, 0.6930751788318458, 0.6931267004201329, 0.6931426805531953, 0.6931356605157082, 0.6931459005593992, 0.6931336604990145, 0.6930991597912757, 0.693031656111244, 0.6931303604656399, 0.6928245658650388, 0.6931455605590705, 0.6929957929203853, 0.6930519575374322, 0.6931466805598621, 0.6931455605590705, 0.6931470005599345, 0.6929704501484872, 0.6930888594261516, 0.6931471605599452, 0.6930775589442072, 0.6928591529055732, 0.6930775589442072, 0.6930930995850211, 0.6931183002819208, 0.6931366005226329, 0.6931279604368074, 0.6931471605599452, 0.6928962795754752, 0.6931291804519437, 0.6931240603817639, 0.6931029999092981, 0.6931084600601837, 0.6930775589442072, 0.693141400548809, 0.6930600580298041, 0.6930491773583609, 0.6930191750980527, 0.6930376565613848, 0.6930821991524053, 0.6930093942314716, 0.6931468605599111, 0.6931443005571805, 0.6930600580298041, 0.6931471605599452, 0.6931471005599432, 0.6931366005226329, 0.6931471005599432, 0.6931066800131775, 0.6931447605579931, 0.6931438005561372, 0.6930491773583609, 0.6929666696982935, 0.6931279604368074, 0.6931212603359918, 0.6930775589442072, 0.6930093942314716, 0.6930463571714478, 0.693137500528711, 0.6930888594261516, 0.693139960542569, 0.6931315004779901, 0.6931383605340142, 0.693122680359858, 0.6931462005596252, 0.6931315004779901, 0.6931466805598621, 0.6931366005226329, 0.6931391805386118, 0.6930546977088843, 0.6928639338160292, 0.6931212603359918, 0.6930971797265786, 0.6931447605579931, 0.6930751788318458, 0.6931407005459485, 0.6931466805598621, 0.6931466805598621, 0.6930405967731903, 0.6931464605597725, 0.6931119001450408, 0.6931151802186033, 0.693122680359858, 0.693116760251479, 0.6929992532656399, 0.6930677984594171, 0.6928591529055732, 0.6931303604656399, 0.693104859962931, 0.6926091640657721, 0.6931471605599452, 0.6930573978729362, 0.6930727587137232, 0.6931254004018198, 0.6930600580298041, 0.693141400548809, 0.6931315004779901, 0.6931303604656399, 0.6931198003100517, 0.6931151802186033, 0.692974190584538, 0.6931315004779901, 0.6931267004201329, 0.6929628492336543, 0.6931383605340142, 0.6931447605579931, 0.693113560183167, 0.6929051610348317, 0.6931029999092981, 0.6931407005459485, 0.6931471005599432, 0.6931471005599432, 0.6931010998521303, 0.6931279604368074, 0.693141400548809, 0.6931240603817639, 0.6931346805078614, 0.6930223553660826, 0.6931326004890856, 0.6931326004890856, 0.693113560183167, 0.6930930995850211, 0.6931240603817639, 0.6930751788318458, 0.6931455605590705, 0.6931010998521303, 0.6931471005599432, 0.6931471605599452, 0.6930546977088843, 0.6931119001450408, 0.6930727587137232, 0.6931240603817639, 0.6931471605599452, 0.6931291804519437, 0.6931407005459485, 0.6931303604656399, 0.6931455605590705, 0.6931466805598621, 0.6931010998521303, 0.693113560183167, 0.6931267004201329, 0.6931084600601837, 0.6931391805386118, 0.6931198003100517, 0.6931471805599453, 0.6931462005596252, 0.6929922925630163, 0.6931315004779901, 0.6931466805598621, 0.6930930995850211, 0.6931010998521303, 0.6931459005593992, 0.6931468605599111, 0.6931326004890856, 0.6931254004018198, 0.693070298589646, 0.6931455605590705, 0.6931346805078614, 0.692878036412888, 0.6931303604656399, 0.6931426805531953, 0.693137500528711, 0.693086679339803, 0.6931326004890856, 0.6930821991524053, 0.6930405967731903, 0.6931383605340142, 0.6930546977088843, 0.693122680359858, 0.6931183002819208, 0.6931432605548231, 0.6930546977088843, 0.6931279604368074, 0.6931462005596252, 0.6931407005459485, 0.6931426805531953, 0.6930191750980527, 0.6929349855506133, 0.6930223553660826, 0.6930677984594171, 0.693116760251479, 0.6931466805598621, 0.6931407005459485, 0.693141400548809, 0.6931438005561372, 0.6931462005596252, 0.6931198003100517, 0.6929851718108113, 0.6930844592486134, 0.6931183002819208, 0.693137500528711, 0.6931366005226329, 0.6931432605548231, 0.6929778910067453, 0.6931336604990145, 0.6931470005599345, 0.6931464605597725, 0.6930346763408155, 0.693104859962931, 0.6931466805598621, 0.692882657234926, 0.693113560183167, 0.6931471605599452, 0.6930126945310066, 0.6931326004890856, 0.6929390861251135, 0.6931315004779901, 0.692974190584538, 0.69270309481867, 0.693139960542569, 0.6931470005599345, 0.6931212603359918, 0.693079899050998, 0.6929266643503358, 0.6930600580298041, 0.6930573978729362, 0.6931315004779901, 0.6930727587137232, 0.6930491773583609, 0.6931383605340142, 0.6931464605597725, 0.6931366005226329, 0.6931254004018198, 0.6931356605157082, 0.6928543319720637, 0.6931151802186033, 0.6930652583228365, 0.6929922925630163, 0.6931471805599453, 0.693070298589646, 0.6931267004201329, 0.6931183002819208, 0.6930434969764673, 0.6930463571714478, 0.6931240603817639, 0.6930573978729362, 0.693137500528711, 0.6928445700343367, 0.6931426805531953, 0.6930951596578809, 0.6931426805531953, 0.6931471005599432, 0.6930519575374322, 0.6931466805598621, 0.6931468605599111, 0.6930546977088843, 0.6931391805386118, 0.6929589887542638, 0.6931326004890856, 0.6931212603359918, 0.6930971797265786, 0.6931119001450408, 0.6931466805598621, 0.6931291804519437, 0.6931443005571805, 0.6931212603359918, 0.693070298589646, 0.693122680359858, 0.6931010998521303, 0.6931471005599432, 0.6931291804519437, 0.6931459005593992, 0.6929589887542638, 0.6926789274683951, 0.6931303604656399, 0.6931029999092981, 0.6929138824167064, 0.6928245658650388, 0.693113560183167, 0.6931468605599111, 0.6930546977088843, 0.6930821991524053, 0.693145180558612, 0.6930751788318458, 0.6930677984594171, 0.6931267004201329, 0.693139960542569, 0.6931391805386118, 0.6931151802186033, 0.693031656111244, 0.6931426805531953, 0.6931471005599432, 0.6931459005593992, 0.693070298589646, 0.6930951596578809, 0.6931470005599345, 0.6930909995078338, 0.6930191750980527, 0.6930991597912757, 0.693145180558612, 0.6931459005593992, 0.6931254004018198, 0.6931462005596252, 0.6931464605597725, 0.693006053920911, 0.693139960542569, 0.6931346805078614, 0.6931438005561372, 0.6931240603817639, 0.693104859962931, 0.6931459005593992, 0.6930463571714478, 0.693062678179701, 0.6931468605599111, 0.6931447605579931, 0.6931407005459485, 0.6931267004201329, 0.693025495624124, 0.6930346763408155, 0.6930909995078338, 0.6931366005226329, 0.6930951596578809, 0.6930751788318458, 0.6929704501484872, 0.693122680359858, 0.6929051610348317, 0.6929851718108113, 0.6931366005226329, 0.6930971797265786, 0.6931315004779901, 0.6927717535757862, 0.6930677984594171, 0.6931198003100517, 0.6931336604990145, 0.693139960542569, 0.6931240603817639, 0.6931468605599111, 0.6930909995078338, 0.6930888594261516, 0.6931391805386118, 0.692878036412888, 0.6930888594261516, 0.6931471005599432, 0.6931459005593992, 0.6930951596578809, 0.6931462005596252, 0.6930093942314716, 0.693113560183167, 0.6931466805598621, 0.6930930995850211, 0.693079899050998, 0.6930971797265786, 0.6931466805598621, 0.6929815514154054, 0.6930888594261516, 0.6928962795754752, 0.693116760251479, 0.6931407005459485, 0.6930775589442072, 0.6929887521932527, 0.6930751788318458, 0.6931471005599432, 0.6931459005593992, 0.6930346763408155, 0.6929778910067453, 0.6931102001040916, 0.6930652583228365, 0.693141400548809, 0.6931151802186033, 0.6931443005571805, 0.693086679339803, 0.6930600580298041, 0.6931438005561372, 0.693141400548809, 0.693079899050998, 0.6931391805386118, 0.6930677984594171, 0.6931383605340142, 0.6931462005596252, 0.6931336604990145, 0.6931151802186033, 0.6931459005593992, 0.6927933588280369, 0.6927266216000256, 0.6931420605512071, 0.6931471005599432, 0.6930991597912757, 0.6931471605599452, 0.6931198003100517, 0.6931438005561372, 0.6926414753082617, 0.6931336604990145, 0.6931432605548231, 0.6929922925630163, 0.693139960542569, 0.6931267004201329, 0.6931279604368074, 0.6931407005459485, 0.6931279604368074, 0.6931119001450408, 0.693137500528711, 0.6931212603359918, 0.693139960542569, 0.693079899050998, 0.693116760251479, 0.6931303604656399, 0.6931426805531953, 0.6931468605599111, 0.6928872380357927, 0.6930775589442072, 0.6930727587137232, 0.6931267004201329, 0.6931443005571805, 0.6930546977088843, 0.6931383605340142, 0.6930971797265786, 0.693139960542569, 0.6931468605599111, 0.6930434969764673, 0.6931462005596252, 0.6929390861251135, 0.6931443005571805, 0.6931303604656399, 0.6931183002819208, 0.6931466805598621, 0.6931455605590705, 0.6931455605590705, 0.6930930995850211, 0.6928245658650388, 0.6931407005459485, 0.6931303604656399, 0.6931102001040916, 0.6931432605548231, 0.693086679339803, 0.6931315004779901, 0.6931151802186033, 0.6931315004779901, 0.6930951596578809, 0.6929778910067453, 0.6930909995078338, 0.6930652583228365, 0.6931391805386118, 0.693086679339803, 0.6929224437238923, 0.6931267004201329, 0.6931471005599432, 0.6931356605157082, 0.6931459005593992, 0.6931470005599345, 0.6931151802186033, 0.6931084600601837, 0.6930677984594171, 0.6931303604656399, 0.6930026735990573, 0.693079899050998, 0.6931468605599111, 0.693139960542569, 0.6930652583228365, 0.6931356605157082, 0.6931464605597725, 0.6929815514154054, 0.6930491773583609, 0.6930434969764673, 0.6930463571714478, 0.6931471605599452, 0.6931407005459485, 0.6931336604990145, 0.6931471005599432, 0.6931432605548231, 0.6930376565613848, 0.6930909995078338, 0.6931279604368074, 0.693079899050998, 0.6931336604990145, 0.6929778910067453, 0.6931459005593992, 0.6931291804519437, 0.6931438005561372, 0.6928639338160292, 0.6930285958724285, 0.6931066800131775, 0.6930677984594171, 0.693141400548809, 0.693086679339803, 0.6931326004890856, 0.6931315004779901, 0.6931315004779901, 0.693141400548809, 0.693137500528711, 0.6931010998521303, 0.6931407005459485, 0.6930727587137232, 0.6931471605599452, 0.6931336604990145, 0.6930751788318458, 0.6931029999092981, 0.6931407005459485, 0.6928591529055732, 0.6931291804519437, 0.6931470005599345, 0.6930821991524053, 0.6931443005571805, 0.6931468605599111, 0.6931183002819208, 0.6929992532656399, 0.6931455605590705, 0.6931432605548231, 0.6930093942314716, 0.6931326004890856, 0.693139960542569, 0.6931029999092981, 0.6931420605512071, 0.6931254004018198, 0.6930930995850211, 0.6930775589442072, 0.6931151802186033, 0.6931468605599111, 0.6931468605599111, 0.6930971797265786, 0.6931391805386118, 0.6930821991524053, 0.69301595481978, 0.6929266643503358, 0.693031656111244, 0.6931459005593992, 0.6930888594261516, 0.6930652583228365, 0.6928296269450891, 0.693104859962931, 0.6930223553660826, 0.6930546977088843, 0.6930821991524053, 0.6931326004890856, 0.6931336604990145, 0.6931383605340142, 0.6930546977088843, 0.6931303604656399, 0.6931407005459485, 0.693031656111244, 0.693141400548809, 0.6930600580298041, 0.693137500528711, 0.6930519575374322, 0.6930652583228365, 0.6930652583228365, 0.6929628492336543, 0.693139960542569, 0.693113560183167, 0.6930844592486134, 0.6931420605512071, 0.6931279604368074, 0.693139960542569, 0.693139960542569, 0.693145180558612, 0.6930775589442072, 0.6931471605599452, 0.6931254004018198, 0.6930991597912757, 0.6930821991524053, 0.6931471805599453, 0.693145180558612, 0.6931391805386118, 0.692878036412888, 0.6931468605599111, 0.6931151802186033, 0.6931391805386118, 0.6931010998521303, 0.6931447605579931, 0.6931407005459485, 0.693070298589646, 0.6931455605590705, 0.6931462005596252, 0.6931443005571805, 0.6931383605340142, 0.6931356605157082, 0.6931066800131775, 0.6930844592486134, 0.6931240603817639, 0.693145180558612, 0.6931447605579931, 0.6931459005593992, 0.6931443005571805, 0.6931346805078614, 0.6931462005596252, 0.693113560183167, 0.6931471005599432, 0.6931010998521303, 0.6931447605579931, 0.6929138824167064, 0.693104859962931, 0.6931447605579931, 0.6931336604990145, 0.693122680359858, 0.6931383605340142, 0.6931443005571805, 0.6931426805531953, 0.6931459005593992, 0.6931366005226329, 0.6931102001040916, 0.69276071077108, 0.6931466805598621, 0.6931198003100517, 0.693122680359858, 0.693122680359858, 0.6930491773583609, 0.6931303604656399, 0.6931470005599345, 0.693113560183167, 0.6929992532656399, 0.6931291804519437, 0.6931183002819208, 0.6930223553660826, 0.693139960542569, 0.6931279604368074, 0.6931459005593992, 0.693122680359858, 0.6931455605590705, 0.6931420605512071, 0.6931119001450408, 0.6931459005593992, 0.6930775589442072, 0.6930285958724285, 0.6929431466829601, 0.693145180558612, 0.69301595481978, 0.6930909995078338, 0.6930930995850211, 0.6931438005561372, 0.6930909995078338, 0.693113560183167, 0.693145180558612, 0.6931471005599432, 0.6930546977088843, 0.6931346805078614, 0.6931438005561372, 0.693104859962931, 0.6931267004201329, 0.6931432605548231, 0.693137500528711, 0.6931471605599452, 0.6931315004779901, 0.6931212603359918, 0.6931010998521303, 0.6931455605590705, 0.693122680359858, 0.6931183002819208, 0.6930405967731903, 0.6931356605157082, 0.6931346805078614, 0.6931183002819208, 0.6930951596578809, 0.6931420605512071, 0.6931438005561372, 0.693104859962931, 0.6930888594261516, 0.6931443005571805, 0.693062678179701, 0.6931119001450408, 0.6931464605597725, 0.6927826362602763, 0.6931432605548231, 0.6931462005596252, 0.693145180558612, 0.6931455605590705, 0.6931366005226329, 0.6930844592486134, 0.6930991597912757, 0.6930991597912757, 0.693145180558612, 0.6930346763408155, 0.6931426805531953, 0.6930909995078338, 0.6931471805599453, 0.693113560183167, 0.6931102001040916, 0.6931240603817639, 0.693031656111244, 0.6930821991524053, 0.6931326004890856, 0.69301595481978, 0.6930821991524053, 0.6929851718108113, 0.6928639338160292, 0.6931426805531953, 0.6931443005571805, 0.6931084600601837, 0.693079899050998, 0.6931464605597725, 0.6931254004018198, 0.6931084600601837, 0.6929666696982935, 0.693139960542569, 0.6931212603359918, 0.6931432605548231, 0.6929138824167064, 0.6929851718108113, 0.6931326004890856, 0.6930285958724285, 0.6931084600601837, 0.6931468605599111, 0.6930652583228365, 0.6931366005226329, 0.6931102001040916, 0.6931198003100517, 0.693104859962931, 0.6931356605157082, 0.6930821991524053, 0.6931447605579931, 0.6931471805599453, 0.693113560183167, 0.6931315004779901, 0.6930821991524053, 0.6930677984594171, 0.6929704501484872, 0.6931029999092981, 0.6930821991524053, 0.6931267004201329, 0.693070298589646, 0.6929666696982935, 0.6928872380357927, 0.6931420605512071, 0.6929007403150153, 0.6931426805531953, 0.6931254004018198, 0.6931254004018198, 0.6931470005599345, 0.6930844592486134, 0.6931119001450408, 0.6931466805598621, 0.6931254004018198, 0.6930775589442072, 0.6931326004890856, 0.6931407005459485, 0.6928445700343367, 0.693139960542569, 0.6931407005459485, 0.6931326004890856, 0.6931356605157082, 0.693141400548809, 0.693070298589646, 0.6930223553660826, 0.693079899050998, 0.6930519575374322, 0.6931455605590705, 0.6931383605340142, 0.6929666696982935, 0.6930930995850211, 0.6929181830794628, 0.6931356605157082, 0.6931084600601837, 0.693141400548809, 0.6929471672244782, 0.6930546977088843, 0.6930405967731903, 0.6931432605548231, 0.6931466805598621, 0.6931303604656399, 0.6931279604368074, 0.693062678179701, 0.6931447605579931, 0.693141400548809, 0.692882657234926, 0.6931464605597725, 0.693122680359858, 0.6928962795754752, 0.6931462005596252, 0.6930491773583609, 0.6930573978729362, 0.6931151802186033, 0.6931291804519437, 0.6931407005459485, 0.693113560183167, 0.6931432605548231, 0.693139960542569, 0.6930026735990573, 0.6931315004779901, 0.693086679339803, 0.6926157063983429, 0.6928396290293375, 0.6931443005571805, 0.6931391805386118, 0.6931240603817639, 0.6931471005599432, 0.6931102001040916, 0.6931240603817639, 0.6931240603817639, 0.6931151802186033]\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siptest\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179.40543547046323\n",
      "0.5980181182348775\n",
      "1.0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siptest\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\utils\\validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-9399465e7c88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk_sample_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmatrix1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mpvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnullDist\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-36dee12cf108>\u001b[0m in \u001b[0;36muf\u001b[1;34m(X, y, n_estimators, max_samples, base, kappa)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtree_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m# Find the indices of the training set used for partition.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0msampled_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_samples_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtree_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0munsampled_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampled_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\u001b[0m in \u001b[0;36mestimators_samples_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \"\"\"\n\u001b[0;32m    428\u001b[0m         return [sample_indices\n\u001b[1;32m--> 429\u001b[1;33m                 for _, sample_indices in self._get_estimators_indices()]\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mThus\u001b[0m \u001b[0mfetching\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperty\u001b[0m \u001b[0mmay\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mslower\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mexpected\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \"\"\"\n\u001b[1;32m--> 428\u001b[1;33m         return [sample_indices\n\u001b[0m\u001b[0;32m    429\u001b[0m                 for _, sample_indices in self._get_estimators_indices()]\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\u001b[0m in \u001b[0;36m_get_estimators_indices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    409\u001b[0m                 \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbootstrap_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m                 self._max_samples)\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mfeature_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\u001b[0m in \u001b[0;36m_generate_bagging_indices\u001b[1;34m(random_state, bootstrap_features, bootstrap_samples, n_features, n_samples, max_features, max_samples)\u001b[0m\n\u001b[0;32m     55\u001b[0m                                         n_features, max_features)\n\u001b[0;32m     56\u001b[0m     sample_indices = _generate_indices(random_state, bootstrap_samples,\n\u001b[1;32m---> 57\u001b[1;33m                                        n_samples, max_samples)\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeature_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\u001b[0m in \u001b[0;36m_generate_indices\u001b[1;34m(random_state, bootstrap, n_population, n_samples)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         indices = sample_without_replacement(n_population, n_samples,\n\u001b[1;32m---> 41\u001b[1;33m                                              random_state=random_state)\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#every time generate data set, call uf to get test stat = conditional entropy for number of power reps \n",
    "#null distribution: use y vector, numpy.random.binomial(lengthof(y), sum(y)/lengthof(y)), call entropy of vector, put into list\n",
    "\n",
    "#pvalue - num random entropies larger than UF / 1000 (or length of reps from null dist)\n",
    "\n",
    "#power is prob is that when alt is true, we reject null hypothesis, prob we see p value < 0.05 for example\n",
    "#to calculate: 10-100 number of pvalues, proportion of p-values less than 0.05,\n",
    "\n",
    "#in simulation, noise should be True \n",
    "#n = 100, p =1, noise = TRUE\n",
    "#200 x 2 is x\n",
    "#0s or 1s that tell you whether from first multimodal or shifted multimodal \n",
    "\n",
    "#1000x entropy(numpy.random.binomial(200, 0.5)) add to list \n",
    "\n",
    "from hyppo.sims import ksample_sim\n",
    "from hyppo.ksample._utils import k_sample_transform\n",
    "\n",
    "nullDist = []\n",
    "success= 0\n",
    "for i in range(1000): \n",
    "    temp = []\n",
    "    #nullDist.append(entropy(np.random.binomial(1, 0.5, size = (200,))))\n",
    "    #print(np.random.binomial(1, 0.5, size = 200))\n",
    "    success = np.random.binomial(10000, 0.5)\n",
    "    temp.append(success/10000)\n",
    "    temp.append((10000-success)/10000)\n",
    "    #print(temp)\n",
    "    #print(entropy(temp, base=2))\n",
    "    nullDist.append(entropy(temp))\n",
    "print(nullDist)\n",
    "\n",
    "power = []\n",
    "\n",
    "for x in SAMP_SIZES: \n",
    "    pvalues = []\n",
    "    for p in range(POWER_REPS): \n",
    "        print(p)\n",
    "        #matrix1, matrix2 = ksample_sim.trans_2samp(multimodal_independence, 100, 1, noise = False, trans = 0.3)\n",
    "        matrix1, matrix2 = ksample_sim.trans_2samp(multimodal_independence, 500, 1, noise = False, trans = 0.3)\n",
    "        \n",
    "        x, y = k_sample_transform([matrix1, matrix2])\n",
    "        ce = uf(x, y)\n",
    "        print(ce)\n",
    "        pvalue = (nullDist >= ce).sum() / 1000\n",
    "        print(pvalue)\n",
    "        pvalues.append(pvalue)\n",
    "    power.append((pvalues >= 0.05).sum() / POWER_REPS)\n",
    "    \n",
    "np.savetxt('C:/Users/siptest/Desktop/hyppo/benchmarks/2samp_vs_samplesize/UF_MultiModal_NotNull.csv',\n",
    "               power, delimiter=',')\n",
    "        \n",
    "        \n",
    "        \n",
    "#from hyppo.sims import ksample_sim\n",
    "#matrix1, matrix2 = ksample_sim.trans_2samp(multimodal_independence, 100, 1, noise=True, trans=0.5)\n",
    "\n",
    "#from hyppo.ksample._utils import k_sample_transform\n",
    "#x, y = k_sample_transform([matrix1, matrix2])\n",
    "\n",
    "#ce = uf(x, y)\n",
    "#calculate p-value, add to list of pvalues, after power rep, see how many are less than 0.05 \n",
    "\n",
    "#sample size, power = number less than 0.05, vector of power, plot \n",
    "#list of powers that append every sample size \n",
    "#plot that to generate curve \n",
    "\n",
    "#power curve should look like growth, should start at something small, should end up being 1 \n",
    "\n",
    "#generate data using trans/rot 2 samp \n",
    "\n",
    "#POWER-REPS = 20 at least \n",
    "\n",
    "#for i in POWER_REPS: \n",
    "    #compute p-value and store into list\n",
    "    \n",
    "#find how many p-values < 0.05 \n",
    "#for every sample size have single number, plot sample size vs numbers, as sample size >, power > \n",
    "#sample szie is outside loop - how smooth \n",
    "\n",
    "#run in 2 cases: null case (no difference between 2 cases): translation value is 0 (trans = 0 in trans_2_samp)\n",
    "#run when translation value is 0.5 start there \n",
    "\n",
    "#2 csv's, one for null and one for alternative, numpy save\n",
    "#also tell file if null or alternative \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_power():\n",
    "    fig, ax = plt.subplots(nrows=4, ncols=5, figsize=(28,24))\n",
    "    \n",
    "    sim_title = [\n",
    "        \"Linear\",\n",
    "        \"Multiplicative\",\n",
    "        \"Independence\"\n",
    "    ]\n",
    "    \n",
    "    for i, row in enumerate(ax):\n",
    "        for j, col in enumerate(row):\n",
    "            count = 5*i + j\n",
    "            sim = simulations[count]\n",
    "            #if null, trans = 0\n",
    "            #if null, trans = 0.5 basically have to find null or not null \n",
    "            \n",
    "            for test in tests:\n",
    "                #mgc_power = np.genfromtxt('C:/Users/siptest/Desktop/hyppo/benchmarks/2samp_vs_samplesize/{}_MGC.csv'.format(sim.__name__),\n",
    "                                          delimiter=',')\n",
    "                power = np.genfromtxt('C:/Users/siptest/Desktop/hyppo/benchmarks/2samp_vs_samplesize/{}_{}.csv'.format(sim.__name__, test.__name__),\n",
    "                                      delimiter=',')\n",
    "                \n",
    "                custom_color = {\n",
    "                    \"Dcorr\" : \"#377eb8\",\n",
    "                    \"Hsic\" : \"#4daf4a\",\n",
    "                    \"MGC\" : \"#e41a1c\",\n",
    "                }\n",
    "                if test.__name__ in custom_color.keys():\n",
    "                    if test.__name__ == \"UF\":\n",
    "                        col.plot(SAMP_SIZES, power - mgc_power, custom_color[test.__name__], label=test.__name__, lw=5)\n",
    "                    else:\n",
    "                        col.plot(SAMP_SIZES, power - mgc_power, custom_color[test.__name__], label=test.__name__, lw=2)\n",
    "                else:\n",
    "                    col.plot(SAMP_SIZES, power - mgc_power, label=test.__name__, lw=1)\n",
    "                col.set_xticks([])\n",
    "                if i == 3:\n",
    "                    col.set_xticks([SAMP_SIZES[0], SAMP_SIZES[-1]])\n",
    "                col.set_ylim(-1.05, 1.05)\n",
    "                col.set_yticks([])\n",
    "                if j == 0:\n",
    "                    col.set_yticks([-1, 0, 1])\n",
    "                col.set_title(sim_title[count])\n",
    "    \n",
    "    fig.text(0.5, 0.08, 'Sample Size', ha='center')\n",
    "    fig.text(0.08, 0.5, 'Statistical Power Relative to MGC', va='center', rotation='vertical')\n",
    "    leg = plt.legend(bbox_to_anchor=(0.5, 0.08), bbox_transform=plt.gcf().transFigure,\n",
    "                     ncol=5, loc='upper center')\n",
    "    leg.get_frame().set_linewidth(0.0)\n",
    "    for legobj in leg.legendHandles:\n",
    "        legobj.set_linewidth(5.0)\n",
    "    plt.subplots_adjust(hspace=.50)\n",
    "    plt.savefig('C:/Users/siptest/Desktop/hyppo/benchmarks/figs/2samp_power_sampsize.pdf', transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_power()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
